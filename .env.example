# LLM Relay Configuration
# Copy this file to .env and fill in your API keys

# =============================================================================
# API Keys (optional - you can also use custom/Ollama providers via Admin UI)
# =============================================================================

# First-party providers
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
GOOGLE_API_KEY=
DEEPSEEK_API_KEY=
MISTRAL_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=

# Aggregators
OPENROUTER_API_KEY=
PERPLEXITY_API_KEY=

# Inference providers
FIREWORKS_API_KEY=
TOGETHER_API_KEY=
DEEPINFRA_API_KEY=
CEREBRAS_API_KEY=
SAMBANOVA_API_KEY=
COHERE_API_KEY=

# =============================================================================
# Admin UI
# =============================================================================

# Password for Admin UI (random generated if not set)
ADMIN_PASSWORD=

# Set to "false" to disable Admin UI entirely
# ADMIN_ENABLED=true

# =============================================================================
# Server Configuration (defaults shown)
# =============================================================================

# API Server
# PORT=11434
# HOST=0.0.0.0

# Admin UI
# ADMIN_PORT=8080
# ADMIN_HOST=0.0.0.0

# Debug logging
# DEBUG=false

# =============================================================================
# Database (optional - defaults to SQLite)
# =============================================================================

# Use PostgreSQL instead of SQLite for production/multi-instance deployments
# DATABASE_URL=postgresql://user:password@host:5432/llm_proxy
