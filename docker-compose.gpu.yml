# LLM Relay - GPU Support (Local Build)
#
# Builds with PyTorch CUDA 12.6 for Pascal GPU support (sm_61).
# The published ghcr.io image uses cu128 which dropped Pascal support.
#
# Enables NVIDIA GPU acceleration for local ML models:
#   - Embeddings (sentence-transformers)
#   - Cross-encoder reranking
#   - Docling document parsing
#
# Requirements:
#   - NVIDIA GPU with CUDA support (Pascal or newer: GTX 10xx, Quadro P-series, etc.)
#   - NVIDIA drivers 525.60.13+ installed on host
#   - NVIDIA Container Toolkit (nvidia-container-toolkit)
#
# Usage (build and run with GPU):
#   docker compose -f docker-compose.gpu.yml up -d --build
#
# Verify GPU is working:
#   docker exec llm-relay-gpu python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"

services:
    llm-relay:
        build:
            context: .
            dockerfile: Dockerfile.gpu
        container_name: llm-relay-gpu
        ports:
            - "11434:11434"
            - "8080:8080"
        volumes:
            - llm-relay-data:/data
        env_file:
            - .env
        runtime: nvidia
        environment:
            - NVIDIA_VISIBLE_DEVICES=all
            - NVIDIA_DRIVER_CAPABILITIES=compute,utility
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu, compute, utility]
        restart: unless-stopped

volumes:
    llm-relay-data:
