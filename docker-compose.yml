# Multi-Provider LLM Proxy
# Standalone Docker Compose configuration
#
# Supported providers (set at least one API key):
#   - Anthropic Claude: claude-opus, claude-sonnet, claude-haiku
#   - OpenAI GPT: gpt-4o, gpt-4o-mini, gpt-4-turbo, o1
#   - Google Gemini: gemini, gemini-pro, gemini-flash
#   - Perplexity: sonar, sonar-pro, sonar-reasoning
#
# Usage:
#   export ANTHROPIC_API_KEY="sk-ant-..."
#   export OPENAI_API_KEY="sk-..."        # optional
#   export GOOGLE_API_KEY="AIza..."       # optional
#   export PERPLEXITY_API_KEY="pplx-..."  # optional
#   docker compose up -d
#
# For Docker Swarm deployments, use docker-compose.swarm.yml instead.

services:
  llm-proxy:
    image: ghcr.io/benhumphry/ollama-llm-proxy:latest
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "11434:11434"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY:-}
      - PORT=11434
      - HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:11434/')",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
